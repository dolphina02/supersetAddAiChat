#!/usr/bin/env python3
"""
Streaming MCP Client for Superset AI Assistant

ì •ì‹ MCP í”„ë¡œí† ì½œì„ ì‚¬ìš©í•œ í´ë¼ì´ì–¸íŠ¸
- MCP HTTP í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
- Server-Sent Events (SSE) ê¸°ë°˜ ì‹¤ì‹œê°„ ì‘ë‹µ
- OpenRouter ìŠ¤íŠ¸ë¦¬ë° API ì—°ë™
"""

import asyncio
import json
import logging
import os
from datetime import datetime
from typing import Any, AsyncGenerator, Dict, List, Optional

import httpx
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
import uvicorn

# OpenAI ê³µì‹ í´ë¼ì´ì–¸íŠ¸ (OpenRouter ì§€ì›)
from openai import AsyncOpenAI

# MCP ì •ì‹ í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
from mcp import ClientSession
from mcp.client.session import ClientSession as MCPClientSession
from mcp.types import (
    CallToolRequest,
    ListToolsRequest,
    Tool,
    TextContent,
    ImageContent,
    EmbeddedResource,
)

# Configure structured logging - ë¡œê·¸ ë ˆë²¨ì„ WARNINGìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ë¶ˆí•„ìš”í•œ ë¡œê·¸ ì¤„ì´ê¸°
logging.basicConfig(
    level=logging.WARNING,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ê°œë°œ ëª¨ë“œì—ì„œë§Œ ìƒì„¸ ë¡œê¹… í™œì„±í™”
if os.getenv("DEBUG", "false").lower() == "true":
    logger.setLevel(logging.DEBUG)
else:
    logger.setLevel(logging.WARNING)

app = FastAPI(title="Streaming Superset MCP Client", version="2.0.0")

# CORS middleware for frontend integration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY", "")
OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"
MCP_SERVER_URL = os.getenv("MCP_SERVER_URL", "http://superset:5008")
DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "openai/gpt-4o-mini")

# ì‹œì‘ ì‹œì—ë§Œ ê¸°ë³¸ ì •ë³´ ë¡œê¹… (WARNING ë ˆë²¨ë¡œ ë³€ê²½í•˜ì—¬ í•­ìƒ í‘œì‹œ)
logger.warning(f"ğŸš€ MCP Client ì‹œì‘ - OpenRouter: {'âœ…' if OPENROUTER_API_KEY else 'âŒ'}, MCP: {MCP_SERVER_URL}, Model: {DEFAULT_MODEL}")

# Pydantic models
class ChatMessage(BaseModel):
    role: str
    content: str
    tool_calls: Optional[List[Dict]] = None

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    model: Optional[str] = DEFAULT_MODEL
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 2000

class StreamChunk(BaseModel):
    """Standard streaming chunk format"""
    type: str  # "progress", "tool_start", "tool_result", "content", "error", "done"
    content: Optional[str] = None
    tool_name: Optional[str] = None
    tool_result: Optional[Dict] = None
    error: Optional[str] = None
    metadata: Optional[Dict] = None
    timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat())

class StreamingMCPClient:
    """ì •ì‹ MCP í”„ë¡œí† ì½œì„ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, server_url: str, server_name: str = "superset"):
        self.server_url = server_url
        self.server_name = server_name
        self._session: Optional[MCPClientSession] = None
        self._tools_cache: Optional[List[Tool]] = None
        # ì´ˆê¸°í™” ë¡œê·¸ ì œê±° (ë¶ˆí•„ìš”í•œ ë¡œê·¸)
    
    async def _get_session(self) -> Optional[MCPClientSession]:
        """MCP ì„¸ì…˜ ìƒì„± ë˜ëŠ” ë°˜í™˜ (í˜„ì¬ëŠ” None ë°˜í™˜ìœ¼ë¡œ HTTP fallback ì‚¬ìš©)"""
        # ì„ì‹œë¡œ MCP ì„¸ì…˜ ìƒì„±ì„ ë¹„í™œì„±í™”í•˜ê³  HTTP fallbackë§Œ ì‚¬ìš©
        # ë¡œê·¸ ì œê±° - ë§¤ë²ˆ í˜¸ì¶œë  ë•Œë§ˆë‹¤ ë¡œê·¸ê°€ ì°íˆëŠ” ê²ƒì„ ë°©ì§€
        return None
    
    async def stream_tool_execution(self, tool_name: str, arguments: Dict[str, Any]) -> AsyncGenerator[StreamChunk, None]:
        """ì •ì‹ MCP í”„ë¡œí† ì½œë¡œ ë„êµ¬ ì‹¤í–‰"""
        
        # ì‹œì‘ ì•Œë¦¼
        yield StreamChunk(
            type="tool_start",
            tool_name=tool_name,
            content=f"ğŸ”§ Executing {tool_name}...",
            metadata={"arguments": arguments}
        )
        
        try:
            session = await self._get_session()
            
            if session:
                # ì •ì‹ MCP í”„ë¡œí† ì½œ ì‚¬ìš©
                # ë¡œê·¸ ì œê±° - ë„êµ¬ í˜¸ì¶œ ì‹œë§ˆë‹¤ ë¡œê·¸ê°€ ì°íˆëŠ” ê²ƒì„ ë°©ì§€
                
                # ì§„í–‰ ìƒí™© ì•Œë¦¼
                yield StreamChunk(
                    type="progress",
                    tool_name=tool_name,
                    content=f"ğŸ“Š Processing {tool_name} via MCP...",
                    metadata={"status": "processing"}
                )
                
                # MCP ë„êµ¬ í˜¸ì¶œ
                request = CallToolRequest(
                    method="tools/call",
                    params={
                        "name": tool_name,
                        "arguments": arguments
                    }
                )
                
                result = await session.call_tool(request)
                # ë¡œê·¸ ì œê±° - ê²°ê³¼ ë¡œê¹… ë¶ˆí•„ìš”
                
                # ê²°ê³¼ ì²˜ë¦¬
                content = self._extract_mcp_content(result)
                
                # Check if data was truncated and add appropriate messaging
                metadata = {"protocol": "mcp"}
                if isinstance(content, dict) and content.get("_truncated"):
                    metadata["data_truncated"] = True
                    metadata["truncation_info"] = {
                        "original_count": content.get("_original_count"),
                        "showing_count": content.get("_showing_count"),
                        "message": content.get("_truncation_message")
                    }
                
                yield StreamChunk(
                    type="tool_result",
                    tool_name=tool_name,
                    content=f"âœ… {tool_name} completed via MCP",
                    tool_result={"content": content, "mcp_result": result},
                    metadata=metadata
                )
            else:
                # Fallback: HTTP ì§ì ‘ í˜¸ì¶œ
                # ë¡œê·¸ ì œê±° - HTTP fallback ì‚¬ìš© ì‹œë§ˆë‹¤ ë¡œê·¸ê°€ ì°íˆëŠ” ê²ƒì„ ë°©ì§€
                
                yield StreamChunk(
                    type="progress",
                    tool_name=tool_name,
                    content=f"ğŸ“Š Processing {tool_name} via HTTP...",
                    metadata={"status": "processing", "fallback": True}
                )
                
                # HTTP ì§ì ‘ í˜¸ì¶œ - MCP JSON-RPC í”„ë¡œí† ì½œ ì‚¬ìš©
                async with httpx.AsyncClient() as client:
                    response = await client.post(
                        f"{self.server_url}/mcp",
                        headers={
                            "Content-Type": "application/json",
                            "Accept": "application/json, text/event-stream"
                        },
                        json={
                            "jsonrpc": "2.0",
                            "id": 1,
                            "method": "tools/call",
                            "params": {
                                "name": tool_name,
                                "arguments": arguments
                            }
                        },
                        timeout=30.0
                    )
                    
                    if response.status_code == 200:
                        # SSE ì‘ë‹µ íŒŒì‹±
                        response_text = response.text.strip()
                        logger.debug(f"Tool call response: {response_text}")
                        
                        # SSE í˜•ì‹ íŒŒì‹±: "event: message\ndata: {json}"
                        if "event: message" in response_text and "data: " in response_text:
                            lines = response_text.split('\n')
                            
                            # ëª¨ë“  SSE ë©”ì‹œì§€ë¥¼ íŒŒì‹±í•˜ì—¬ ì‹¤ì œ ê²°ê³¼ë¥¼ ì°¾ê¸°
                            for i, line in enumerate(lines):
                                if line.startswith("data: "):
                                    json_data = line[6:]  # "data: " ì œê±°
                                    try:
                                        data = json.loads(json_data)
                                        # ì‹¤ì œ ê²°ê³¼ê°€ ìˆëŠ” ë©”ì‹œì§€ë§Œ ì²˜ë¦¬ (ë””ë²„ê·¸ ì•Œë¦¼ ë¬´ì‹œ)
                                        if "result" in data and data.get("jsonrpc") == "2.0":
                                            result = data["result"]
                                            # MCP ê²°ê³¼ì—ì„œ ì‹¤ì œ ì½˜í…ì¸  ì¶”ì¶œ
                                            content = self._extract_mcp_content(result)
                                            
                                            # Check if data was truncated and add appropriate messaging
                                            metadata = {"protocol": "http"}
                                            if isinstance(content, dict) and content.get("_truncated"):
                                                metadata["data_truncated"] = True
                                                metadata["truncation_info"] = {
                                                    "original_count": content.get("_original_count"),
                                                    "showing_count": content.get("_showing_count"),
                                                    "message": content.get("_truncation_message")
                                                }
                                            
                                            yield StreamChunk(
                                                type="tool_result",
                                                tool_name=tool_name,
                                                content=f"âœ… {tool_name} completed via HTTP",
                                                tool_result={"content": content, "mcp_result": result},
                                                metadata=metadata
                                            )
                                            return  # ê²°ê³¼ë¥¼ ì°¾ì•˜ìœ¼ë¯€ë¡œ ì¢…ë£Œ
                                    except json.JSONDecodeError:
                                        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ë¼ì¸ ì‹œë„
                                        continue
                            
                            # ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
                            raise Exception(f"No valid result found in SSE response")
                        else:
                            # ì¼ë°˜ JSON ì‘ë‹µ ì²˜ë¦¬ (fallback)
                            try:
                                data = response.json()
                                if "result" in data:
                                    result = data["result"]
                                    content = self._extract_mcp_content(result)
                                    
                                    # Check if data was truncated and add appropriate messaging
                                    metadata = {"protocol": "http"}
                                    if isinstance(content, dict) and content.get("_truncated"):
                                        metadata["data_truncated"] = True
                                        metadata["truncation_info"] = {
                                            "original_count": content.get("_original_count"),
                                            "showing_count": content.get("_showing_count"),
                                            "message": content.get("_truncation_message")
                                        }
                                    
                                    yield StreamChunk(
                                        type="tool_result",
                                        tool_name=tool_name,
                                        content=f"âœ… {tool_name} completed via HTTP",
                                        tool_result={"content": content, "mcp_result": result},
                                        metadata=metadata
                                    )
                                else:
                                    raise Exception(f"No result in response: {data}")
                            except json.JSONDecodeError as e:
                                raise Exception(f"Failed to parse JSON response: {e}")
                    else:
                        raise Exception(f"HTTP call failed: {response.status_code} - {response.text}")
                        
        except Exception as e:
            logger.error(f"Tool execution failed: {e}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            yield StreamChunk(
                type="error",
                tool_name=tool_name,
                error=str(e)
            )
    
    def _extract_mcp_content(self, mcp_result: Any) -> Any:
        """Extract clean content from MCP result without wrapper structure"""
        # First extract the core content from MCP wrapper
        core_content = self._unwrap_mcp_structure(mcp_result)
        
        # Then apply truncation if needed
        return self._truncate_large_data(core_content)
    
    def _unwrap_mcp_structure(self, mcp_result: Any) -> Any:
        """Remove MCP wrapper structure and extract just the data"""
        if isinstance(mcp_result, dict):
            # Extract from structuredContent first
            if "structuredContent" in mcp_result and mcp_result["structuredContent"]:
                return mcp_result["structuredContent"]
            
            # Extract from content array
            elif "content" in mcp_result and mcp_result["content"]:
                content = mcp_result["content"]
                if isinstance(content, list) and len(content) > 0:
                    if isinstance(content[0], dict) and "text" in content[0]:
                        text_content = content[0]["text"]
                        # Try to parse as JSON to get structured data
                        try:
                            return json.loads(text_content)
                        except (json.JSONDecodeError, TypeError):
                            return text_content
                return content
        
        return mcp_result
    
    def _truncate_large_data(self, data: Any, max_rows: int = 50, max_chars: int = 50000) -> Any:
        """Truncate large datasets to prevent context length issues"""
        if isinstance(data, dict):
            # Handle structured data with rows
            if "data" in data and isinstance(data["data"], list):
                original_count = len(data["data"])
                if original_count > max_rows:
                    # Truncate rows and add summary
                    truncated_data = data.copy()
                    truncated_data["data"] = data["data"][:max_rows]
                    truncated_data["_truncated"] = True
                    truncated_data["_original_count"] = original_count
                    truncated_data["_showing_count"] = max_rows
                    truncated_data["_truncation_message"] = f"âš ï¸ ë°ì´í„°ê°€ ë„ˆë¬´ ë§ì•„ {max_rows}ê°œ í–‰ë§Œ í‘œì‹œí•©ë‹ˆë‹¤. (ì „ì²´: {original_count}ê°œ)"
                    return truncated_data
            
            # Handle other dict structures
            result = {}
            for key, value in data.items():
                result[key] = self._truncate_large_data(value, max_rows, max_chars)
            return result
            
        elif isinstance(data, list):
            if len(data) > max_rows:
                # Truncate list and add metadata
                return {
                    "data": data[:max_rows],
                    "_truncated": True,
                    "_original_count": len(data),
                    "_showing_count": max_rows,
                    "_truncation_message": f"âš ï¸ ë°ì´í„°ê°€ ë„ˆë¬´ ë§ì•„ {max_rows}ê°œ í•­ëª©ë§Œ í‘œì‹œí•©ë‹ˆë‹¤. (ì „ì²´: {len(data)}ê°œ)"
                }
            return [self._truncate_large_data(item, max_rows, max_chars) for item in data]
            
        elif isinstance(data, str):
            if len(data) > max_chars:
                return data[:max_chars] + f"\n\nâš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ {max_chars}ìë¡œ ì˜ë ¸ìŠµë‹ˆë‹¤. (ì „ì²´: {len(data)}ì)"
            return data
            
        return data
    
    def _format_tool_result_for_display(self, tool_result: Dict[str, Any]) -> str:
        """Format tool result as user-friendly text with tables"""
        try:
            content = tool_result.get("content", {})
            
            # Handle dashboard lists
            if isinstance(content, dict) and "dashboards" in content:
                dashboards = content["dashboards"]
                if isinstance(dashboards, list) and len(dashboards) > 0:
                    table_lines = ["| ì œëª© | ID | ìƒíƒœ | ìƒì„±ì¼ |", "|------|----|----|--------|"]
                    for dash in dashboards:
                        title = dash.get("dashboard_title", "").replace("|", "\\|")[:30]
                        dash_id = dash.get("id", "")
                        status = "ê³µê°œ" if dash.get("published") else "ë¹„ê³µê°œ"
                        created = dash.get("created_on", "")[:10] if dash.get("created_on") else "-"
                        table_lines.append(f"| {title} | {dash_id} | {status} | {created} |")
                    
                    result = "\n".join(table_lines)
                    if content.get("_truncated"):
                        result += f"\n\nâš ï¸ {content.get('_truncation_message', 'ë°ì´í„°ê°€ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤.')}"
                    return result
            
            # Handle dataset lists  
            elif isinstance(content, dict) and "datasets" in content:
                datasets = content["datasets"]
                if isinstance(datasets, list) and len(datasets) > 0:
                    table_lines = ["| í…Œì´ë¸”ëª… | ID | ë°ì´í„°ë² ì´ìŠ¤ | ìƒì„±ì¼ |", "|---------|----|-----------|---------| "]
                    for dataset in datasets:
                        table_name = dataset.get("table_name", "").replace("|", "\\|")[:25]
                        dataset_id = dataset.get("id", "")
                        db_name = dataset.get("database_name", "").replace("|", "\\|")[:15]
                        created = dataset.get("created_on", "")[:10] if dataset.get("created_on") else "-"
                        table_lines.append(f"| {table_name} | {dataset_id} | {db_name} | {created} |")
                    
                    result = "\n".join(table_lines)
                    if content.get("_truncated"):
                        result += f"\n\nâš ï¸ {content.get('_truncation_message', 'ë°ì´í„°ê°€ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤.')}"
                    return result
            
            # Handle chart lists
            elif isinstance(content, dict) and "charts" in content:
                charts = content["charts"]
                if isinstance(charts, list) and len(charts) > 0:
                    table_lines = ["| ì°¨íŠ¸ëª… | ID | íƒ€ì… | ìƒì„±ì¼ |", "|-------|----|----|--------|"]
                    for chart in charts:
                        chart_name = chart.get("slice_name", "").replace("|", "\\|")[:25]
                        chart_id = chart.get("id", "")
                        viz_type = chart.get("viz_type", "").replace("|", "\\|")[:15]
                        created = chart.get("created_on", "")[:10] if chart.get("created_on") else "-"
                        table_lines.append(f"| {chart_name} | {chart_id} | {viz_type} | {created} |")
                    
                    result = "\n".join(table_lines)
                    if content.get("_truncated"):
                        result += f"\n\nâš ï¸ {content.get('_truncation_message', 'ë°ì´í„°ê°€ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤.')}"
                    return result
            
            # Fallback to JSON for other data types
            return json.dumps(tool_result, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"Error formatting tool result: {e}")
            return json.dumps(tool_result, ensure_ascii=False, indent=2)
    
    async def list_tools(self) -> List[Dict[str, Any]]:
        """ì •ì‹ MCP í”„ë¡œí† ì½œë¡œ ë„êµ¬ ëª©ë¡ ì¡°íšŒ"""
        if self._tools_cache:
            # ìºì‹œëœ ë„êµ¬ ë°˜í™˜ ì‹œ ë¡œê·¸ ì œê±°
            return [{"name": tool.name, "description": tool.description} for tool in self._tools_cache]
            
        try:
            session = await self._get_session()
            
            if session:
                # ì •ì‹ MCP í”„ë¡œí† ì½œ ì‚¬ìš©
                # ë¡œê·¸ ì œê±° - ë„êµ¬ ëª©ë¡ ì¡°íšŒ ì‹œë§ˆë‹¤ ë¡œê·¸ê°€ ì°íˆëŠ” ê²ƒì„ ë°©ì§€
                
                request = ListToolsRequest(method="tools/list")
                response = await session.list_tools(request)
                
                self._tools_cache = response.tools
                tools = [{"name": tool.name, "description": tool.description} for tool in response.tools]
                
                # ë¡œê·¸ ì œê±° - ë„êµ¬ ê°œìˆ˜ ë¡œê¹… ë¶ˆí•„ìš”
                return tools
            else:
                # Fallback: HTTP ì§ì ‘ í˜¸ì¶œ
                logger.debug(f"Attempting to connect to MCP server: {self.server_url}")
                
                async with httpx.AsyncClient() as client:
                    # MCP serverì—ì„œ ì •ì‹ JSON-RPC í”„ë¡œí† ì½œë¡œ ë„êµ¬ ëª©ë¡ ìš”ì²­
                    request_payload = {
                        "jsonrpc": "2.0",
                        "id": 1,
                        "method": "tools/list"
                    }
                    logger.debug(f"Sending MCP request: {request_payload}")
                    
                    response = await client.post(
                        f"{self.server_url}/mcp",
                        headers={
                            "Content-Type": "application/json",
                            "Accept": "application/json, text/event-stream"
                        },
                        json=request_payload,
                        timeout=10.0
                    )
                    
                    logger.debug(f"MCP server response status: {response.status_code}")
                    logger.debug(f"MCP server response headers: {dict(response.headers)}")
                    
                    if response.status_code == 200:
                        # Server-Sent Events ì‘ë‹µ íŒŒì‹±
                        response_text = response.text.strip()
                        logger.debug(f"Raw MCP response: {response_text}")
                        
                        # SSE í˜•ì‹ íŒŒì‹±: "event: message\ndata: {json}"
                        if "event: message" in response_text and "data: " in response_text:
                            lines = response_text.split('\n')
                            
                            # ëª¨ë“  SSE ë©”ì‹œì§€ë¥¼ íŒŒì‹±í•˜ì—¬ ì‹¤ì œ ê²°ê³¼ë¥¼ ì°¾ê¸°
                            for line in lines:
                                if line.startswith("data: "):
                                    json_data = line[6:]  # "data: " ì œê±°
                                    try:
                                        data = json.loads(json_data)
                                        logger.debug(f"Parsed SSE JSON: {data}")
                                        # ì‹¤ì œ ê²°ê³¼ê°€ ìˆëŠ” ë©”ì‹œì§€ë§Œ ì²˜ë¦¬ (ë””ë²„ê·¸ ì•Œë¦¼ ë¬´ì‹œ)
                                        if "result" in data and "tools" in data.get("result", {}) and data.get("jsonrpc") == "2.0":
                                            tools = data["result"]["tools"]
                                            logger.debug(f"Found {len(tools)} tools from MCP server")
                                            return tools
                                    except json.JSONDecodeError as e:
                                        logger.debug(f"Failed to parse SSE JSON: {e}, data: {json_data}")
                                        continue
                        
                        # ì¼ë°˜ JSON ì‘ë‹µ ì²˜ë¦¬ (fallback) - SSEê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì‹œë„
                        elif not response_text.startswith("event:"):
                            try:
                                data = response.json()
                                logger.debug(f"Parsed regular JSON: {data}")
                                if "result" in data and "tools" in data["result"]:
                                    tools = data["result"]["tools"]
                                    logger.debug(f"Found {len(tools)} tools from regular JSON")
                                    return tools
                            except json.JSONDecodeError as e:
                                logger.error(f"Failed to parse regular JSON: {e}")
                                logger.error(f"Response content type: {response.headers.get('content-type')}")
                                logger.error(f"Response text: {response_text[:500]}...")
                    else:
                        logger.error(f"MCP server tools list failed: {response.status_code}")
                        
        except Exception as e:
            logger.error(f"Failed to list tools: {e}")
            if logger.isEnabledFor(logging.DEBUG):
                import traceback
                logger.debug(f"Full traceback: {traceback.format_exc()}")
            
        # MCP ì„œë²„ ì—°ê²° ì‹¤íŒ¨ ì‹œ ë¹ˆ ëª©ë¡ ë°˜í™˜
        logger.error(f"Failed to connect to MCP server at {self.server_url}")
        return []

class StreamingOpenRouterClient:
    """OpenAI ê³µì‹ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•œ OpenRouter í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        # OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ OpenRouterë¡œ ì„¤ì •
        self.client = AsyncOpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
        )
        # ì´ˆê¸°í™” ë¡œê·¸ ì œê±°
    
    def _format_tool_result_for_display(self, tool_result: Dict[str, Any]) -> str:
        """Format tool result as user-friendly text with tables"""
        try:
            content = tool_result.get("content", {})
            
            # Handle dashboard lists
            if isinstance(content, dict) and "dashboards" in content:
                dashboards = content["dashboards"]
                if isinstance(dashboards, list) and len(dashboards) > 0:
                    table_lines = ["| ì œëª© | ID | ìƒíƒœ | ìƒì„±ì¼ |", "|------|----|----|--------|"]
                    for dash in dashboards:
                        title = dash.get("dashboard_title", "").replace("|", "\\|")[:30]
                        dash_id = dash.get("id", "")
                        status = "ê³µê°œ" if dash.get("published") else "ë¹„ê³µê°œ"
                        created = dash.get("created_on", "")[:10] if dash.get("created_on") else "-"
                        table_lines.append(f"| {title} | {dash_id} | {status} | {created} |")
                    
                    result = "\n".join(table_lines)
                    if content.get("_truncated"):
                        result += f"\n\nâš ï¸ {content.get('_truncation_message', 'ë°ì´í„°ê°€ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤.')}"
                    return result
            
            # Handle dataset lists  
            elif isinstance(content, dict) and "datasets" in content:
                datasets = content["datasets"]
                if isinstance(datasets, list) and len(datasets) > 0:
                    table_lines = ["| í…Œì´ë¸”ëª… | ID | ë°ì´í„°ë² ì´ìŠ¤ | ìƒì„±ì¼ |", "|---------|----|-----------|---------| "]
                    for dataset in datasets:
                        table_name = dataset.get("table_name", "").replace("|", "\\|")[:25]
                        dataset_id = dataset.get("id", "")
                        db_name = dataset.get("database_name", "").replace("|", "\\|")[:15]
                        created = dataset.get("created_on", "")[:10] if dataset.get("created_on") else "-"
                        table_lines.append(f"| {table_name} | {dataset_id} | {db_name} | {created} |")
                    
                    result = "\n".join(table_lines)
                    if content.get("_truncated"):
                        result += f"\n\nâš ï¸ {content.get('_truncation_message', 'ë°ì´í„°ê°€ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤.')}"
                    return result
            
            # Handle chart lists
            elif isinstance(content, dict) and "charts" in content:
                charts = content["charts"]
                if isinstance(charts, list) and len(charts) > 0:
                    table_lines = ["| ì°¨íŠ¸ëª… | ID | íƒ€ì… | ìƒì„±ì¼ |", "|-------|----|----|--------|"]
                    for chart in charts:
                        chart_name = chart.get("slice_name", "").replace("|", "\\|")[:25]
                        chart_id = chart.get("id", "")
                        viz_type = chart.get("viz_type", "").replace("|", "\\|")[:15]
                        created = chart.get("created_on", "")[:10] if chart.get("created_on") else "-"
                        table_lines.append(f"| {chart_name} | {chart_id} | {viz_type} | {created} |")
                    
                    result = "\n".join(table_lines)
                    if content.get("_truncated"):
                        result += f"\n\nâš ï¸ {content.get('_truncation_message', 'ë°ì´í„°ê°€ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤.')}"
                    return result
            
            # Fallback to JSON for other data types
            return json.dumps(tool_result, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"Error formatting tool result: {e}")
            return json.dumps(tool_result, ensure_ascii=False, indent=2)
    
    def _convert_mcp_tool_to_openai_function(self, mcp_tool: Dict[str, Any]) -> Dict[str, Any]:
        """Convert MCP tool schema to OpenAI function format"""
        # MCP serverì—ì„œ ì œê³µí•˜ëŠ” ì‹¤ì œ input schema ì‚¬ìš© (í•„ìˆ˜)
        if "inputSchema" not in mcp_tool:
            logger.error(f"Tool {mcp_tool.get('name', 'unknown')} missing inputSchema")
            raise ValueError(f"Tool {mcp_tool.get('name', 'unknown')} must have inputSchema")
        
        return {
            "type": "function",
            "function": {
                "name": mcp_tool["name"],
                "description": mcp_tool["description"],
                "parameters": mcp_tool["inputSchema"]
            }
        }
    
    async def stream_chat_with_tools(
        self, 
        messages: List[Dict[str, Any]], 
        tools: List[Dict[str, Any]], 
        model: str,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> AsyncGenerator[StreamChunk, None]:
        """OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…"""
        
        if not self.api_key:
            yield StreamChunk(type="error", error="OpenRouter API key not configured")
            return
        
        # Convert MCP tools to OpenAI format
        openai_tools = [self._convert_mcp_tool_to_openai_function(tool) for tool in tools] if tools else None
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ë¡œê·¸ ì œê±° - ë§¤ ìš”ì²­ë§ˆë‹¤ ë¡œê·¸ê°€ ì°íˆëŠ” ê²ƒì„ ë°©ì§€
        
        try:
            # OpenAI í´ë¼ì´ì–¸íŠ¸ë¡œ ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­
            kwargs = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": True,
                "extra_headers": {
                    "HTTP-Referer": "http://localhost:8088",
                    "X-Title": "Superset AI Assistant",
                }
            }
            
            # ë„êµ¬ê°€ ìˆì„ ë•Œë§Œ ì¶”ê°€
            if openai_tools:
                kwargs["tools"] = openai_tools
                kwargs["tool_choice"] = "auto"
            
            # ìš”ì²­ íŒŒë¼ë¯¸í„° ë¡œê·¸ ì œê±°
            
            stream = await self.client.chat.completions.create(**kwargs)
            
            tool_calls = []
            chunk_count = 0
            
            async for chunk in stream:
                chunk_count += 1
                # ëª¨ë“  ì²­í¬ ë¡œê¹… ì œê±° - ì´ê²ƒì´ ì£¼ìš” ë¡œê·¸ ìŠ¤íŒ¸ ì›ì¸
                
                if not chunk.choices:
                    # ë¹ˆ choices ë¡œê·¸ ì œê±°
                    continue
                    
                choice = chunk.choices[0]
                delta = choice.delta
                
                # Deltaì™€ finish reason ë¡œê·¸ ì œê±°
                
                # ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¬ë°
                if delta.content:
                    # ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¬ë° ë¡œê·¸ ì œê±° - ë§¤ ì²­í¬ë§ˆë‹¤ ë¡œê·¸ê°€ ì°íˆëŠ” ê²ƒì„ ë°©ì§€
                    yield StreamChunk(
                        type="content",
                        content=delta.content
                    )
                
                # ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬
                if delta.tool_calls:
                    # ë„êµ¬ í˜¸ì¶œ ë°œê²¬ ë¡œê·¸ ì œê±°
                    
                    # ë„êµ¬ í˜¸ì¶œ ëˆ„ì  (OpenAIëŠ” ì²­í¬ë³„ë¡œ ë¶€ë¶„ ì „ì†¡)
                    for tool_call_delta in delta.tool_calls:
                        # ìƒˆë¡œìš´ ë„êµ¬ í˜¸ì¶œì¸ì§€ í™•ì¸
                        while len(tool_calls) <= tool_call_delta.index:
                            tool_calls.append({
                                "id": "",
                                "type": "function",
                                "function": {"name": "", "arguments": ""}
                            })
                        
                        # ë„êµ¬ í˜¸ì¶œ ì •ë³´ ëˆ„ì 
                        if tool_call_delta.id:
                            tool_calls[tool_call_delta.index]["id"] = tool_call_delta.id
                        if tool_call_delta.function:
                            if tool_call_delta.function.name:
                                tool_calls[tool_call_delta.index]["function"]["name"] += tool_call_delta.function.name
                            if tool_call_delta.function.arguments:
                                tool_calls[tool_call_delta.index]["function"]["arguments"] += tool_call_delta.function.arguments
                
                # ì™„ë£Œ ì²˜ë¦¬
                if choice.finish_reason == "tool_calls":
                    # ë„êµ¬ í˜¸ì¶œ ì™„ë£Œ ë¡œê·¸ ì œê±°
                    yield StreamChunk(
                        type="tool_calls",
                        metadata={"tool_calls": tool_calls}
                    )
                    return
                elif choice.finish_reason == "stop":
                    # ì •ìƒ ì™„ë£Œ ë¡œê·¸ ì œê±°
                    yield StreamChunk(type="done")
                    return
            
            # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ë¡œê·¸ ì œê±°
            yield StreamChunk(type="done")
                    
        except Exception as e:
            error_str = str(e)
            # ì—ëŸ¬ë§Œ ë¡œê¹…í•˜ê³  ìƒì„¸ tracebackì€ DEBUG ëª¨ë“œì—ì„œë§Œ
            logger.error(f"OpenAI ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
            if logger.isEnabledFor(logging.DEBUG):
                import traceback
                logger.debug(f"Full traceback: {traceback.format_exc()}")
            
            # Context length ì—ëŸ¬ ì²˜ë¦¬
            if "context length" in error_str.lower() or "maximum context length" in error_str.lower():
                yield StreamChunk(
                    type="error", 
                    error="âš ï¸ ë°ì´í„°ê°€ ë„ˆë¬´ ë§ì•„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì¡°ê±´ìœ¼ë¡œ í•„í„°ë§í•˜ê±°ë‚˜ ì‘ì€ ë²”ìœ„ì˜ ë°ì´í„°ë¥¼ ìš”ì²­í•´ì£¼ì„¸ìš”.",
                    metadata={"error_type": "context_length_exceeded", "original_error": error_str}
                )
            else:
                yield StreamChunk(type="error", error=str(e))

# Initialize clients
streaming_mcp_client = StreamingMCPClient(MCP_SERVER_URL, "superset")

try:
    streaming_openrouter_client = StreamingOpenRouterClient(OPENROUTER_API_KEY)
    # ì´ˆê¸°í™” ì„±ê³µ ë¡œê·¸ ì œê±°
except Exception as e:
    logger.error(f"StreamingOpenRouterClient ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    if logger.isEnabledFor(logging.DEBUG):
        import traceback
        logger.debug(f"Traceback: {traceback.format_exc()}")
    streaming_openrouter_client = None

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "version": "2.0.0-streaming",
        "mcp_server": MCP_SERVER_URL,
        "streaming_enabled": True
    }

@app.get("/models")
async def list_models():
    """List available models"""
    recommended_models = [
        {
            "id": "openai/gpt-4o-mini",
            "name": "GPT-4o Mini",
            "description": "Fast and efficient with function calling support",
            "supports_functions": True,
            "supports_streaming": True,
            "cost": "Low"
        },
        {
            "id": "openai/gpt-4o",
            "name": "GPT-4o",
            "description": "Most capable model with excellent function calling",
            "supports_functions": True,
            "supports_streaming": True,
            "cost": "Medium"
        },
        {
            "id": "anthropic/claude-3.5-sonnet",
            "name": "Claude 3.5 Sonnet",
            "description": "Excellent reasoning with function calling support",
            "supports_functions": True,
            "supports_streaming": True,
            "cost": "Medium"
        }
    ]
    return {"models": recommended_models, "default": DEFAULT_MODEL}

@app.get("/mcp/tools")
async def list_mcp_tools():
    """List available MCP tools"""
    # API í˜¸ì¶œ ë¡œê·¸ ì œê±° - ë§¤ë²ˆ í˜¸ì¶œë  ë•Œë§ˆë‹¤ ë¡œê·¸ê°€ ì°íˆëŠ” ê²ƒì„ ë°©ì§€
    tools = await streaming_mcp_client.list_tools()
    # ë„êµ¬ ê°œìˆ˜ ë¡œê·¸ ì œê±°
    return {"tools": tools, "server": "superset"}

@app.post("/chat")
async def chat_completion_stream(request: ChatRequest):
    """Streaming chat completion with real-time MCP tool integration"""
    
    async def generate_streaming_response() -> AsyncGenerator[str, None]:
        try:
            # OpenRouter í´ë¼ì´ì–¸íŠ¸ ì²´í¬
            if streaming_openrouter_client is None:
                error_chunk = StreamChunk(type="error", error="OpenRouter client not initialized")
                yield f"data: {error_chunk.model_dump_json()}\n\n"
                return
            
            # Get available MCP tools
            mcp_tools = await streaming_mcp_client.list_tools()
            
            # ë„êµ¬ ê°œìˆ˜ì™€ í´ë¼ì´ì–¸íŠ¸ íƒ€ì… ë¡œê·¸ ì œê±°
            
            # Add system message with MCP context
            system_message = {
                "role": "system",
                "content": f"""You are a Superset AI Assistant with access to powerful MCP tools for data visualization and analysis.

ğŸ¯ CRITICAL: You MUST use the available tools to provide accurate, real-time data from Superset. NEVER make up or assume data.

ğŸ“Š Available tools: {', '.join([tool.get('name', 'unknown') for tool in mcp_tools])}

ğŸ’¡ Always call tools to get current data before responding. Provide specific, actionable information.

ï¿½ MANDATRORY TABLE FORMAT RULES:
1. NEVER show raw JSON data to users
2. ALWAYS convert data arrays to markdown tables
3. When you receive tool results with arrays like "dashboards", "charts", "datasets", you MUST format them as tables
4. Do NOT show metadata like count, page_size, total_count, pagination info
5. Extract ONLY the core data array and present it as a clean table

ğŸ“‹ REQUIRED TABLE FORMAT:
For dashboards:
| ì œëª© | ID | ìƒíƒœ | ìƒì„±ì¼ | UUID |
|------|----|----|--------|------|
| [title] | [id] | [published status] | [date] | [uuid] |

For datasets:
| í…Œì´ë¸”ëª… | ID | ë°ì´í„°ë² ì´ìŠ¤ | ìƒì„±ì¼ | UUID |
|---------|----|-----------|---------|----|
| [table_name] | [id] | [database_name] | [date] | [uuid] |

For charts:
| ì°¨íŠ¸ëª… | ID | íƒ€ì… | ìƒì„±ì¼ | UUID |
|-------|----|----|--------|------|
| [slice_name] | [id] | [viz_type] | [date] | [uuid] |

ğŸ”¥ FORMATTING RULES:
- Dates: Show as YYYY-MM-DD only
- Boolean published: "ê³µê°œ" for true, "ë¹„ê³µê°œ" for false  
- NULL values: Show as "-"
- Long text: Truncate to 30 characters with "..."
- NEVER show raw JSON objects or arrays

ğŸš¨ EXAMPLE TRANSFORMATION:
If tool returns: {{"dashboards": [{{"id": 1, "dashboard_title": "Sales", "published": true}}]}}
You MUST show:
| ì œëª© | ID | ìƒíƒœ |
|------|----|----|
| Sales | 1 | ê³µê°œ |

NOT the raw JSON!"""
            }
            
            # Prepare messages
            messages = [system_message] + [{"role": msg.role, "content": msg.content} for msg in request.messages]
            
            # Check if model supports function calling
            model_supports_functions = request.model in [
                "openai/gpt-4o-mini", "openai/gpt-4o", "openai/gpt-3.5-turbo",
                "anthropic/claude-3.5-sonnet", "anthropic/claude-3-opus"
            ]
            
            # ì‹¤ì œ OpenAI ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            if model_supports_functions and mcp_tools:
                # Stream with function calling
                async for chunk in streaming_openrouter_client.stream_chat_with_tools(
                    messages, mcp_tools, request.model, request.temperature, request.max_tokens
                ):
                    if chunk.type == "tool_calls":
                        # Execute tools and continue streaming
                        tool_calls = chunk.metadata["tool_calls"]
                        
                        # Execute each tool with streaming progress
                        tool_results = []
                        for tool_call in tool_calls:
                            function_name = tool_call["function"]["name"]
                            try:
                                function_args_str = tool_call["function"]["arguments"]
                                # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ê³µë°±ë§Œ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
                                if not function_args_str or not function_args_str.strip():
                                    function_args = {}
                                else:
                                    function_args = json.loads(function_args_str)
                            except (json.JSONDecodeError, KeyError, TypeError) as e:
                                logger.error(f"Failed to parse tool arguments: {e}, raw: {tool_call}")
                                function_args = {}
                            
                            async for tool_chunk in streaming_mcp_client.stream_tool_execution(function_name, function_args):
                                yield f"data: {tool_chunk.model_dump_json()}\n\n"
                                
                                if tool_chunk.type == "tool_result":
                                    # Format tool result for better presentation
                                    formatted_content = self._format_tool_result_for_display(tool_chunk.tool_result)
                                    tool_results.append({
                                        "tool_call_id": tool_call["id"],
                                        "role": "tool",
                                        "content": formatted_content
                                    })
                        
                        # Continue with tool results
                        messages_with_results = messages + [
                            {"role": "assistant", "tool_calls": tool_calls}
                        ] + tool_results
                        
                        # Stream final response
                        async for final_chunk in streaming_openrouter_client.stream_chat_with_tools(
                            messages_with_results, [], request.model, request.temperature, request.max_tokens
                        ):
                            yield f"data: {final_chunk.model_dump_json()}\n\n"
                        
                        return
                    else:
                        yield f"data: {chunk.model_dump_json()}\n\n"
            else:
                # Stream without function calling
                async for chunk in streaming_openrouter_client.stream_chat_with_tools(
                    messages, [], request.model, request.temperature, request.max_tokens
                ):
                    yield f"data: {chunk.model_dump_json()}\n\n"
            
        except Exception as e:
            logger.error(f"Streaming failed: {e}")
            error_chunk = StreamChunk(type="error", error=str(e))
            yield f"data: {error_chunk.model_dump_json()}\n\n"
    
    return StreamingResponse(
        generate_streaming_response(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )

if __name__ == "__main__":
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)