#!/usr/bin/env python3
"""
Streaming MCP Client for Superset AI Assistant

ì´ ì„œë¹„ìŠ¤ëŠ” Superset AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ í•µì‹¬ ë¸Œë¦¬ì§€ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. Superset í”„ë¡ íŠ¸ì—”ë“œë¡œë¶€í„° ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë°›ìŒ
2. OpenRouter APIë¥¼ í†µí•´ LLMê³¼ í†µì‹ 
3. LLMì´ ìš”ì²­í•œ ë„êµ¬ë¥¼ MCP ì„œë²„ë¥¼ í†µí•´ ì‹¤í–‰
4. ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í”„ë¡ íŠ¸ì—”ë“œì— ì „ë‹¬

ì£¼ìš” ê¸°ëŠ¥:
- MCP Protocol: Model Context Protocolì„ ì‚¬ìš©í•œ ì•ˆì „í•œ ë„êµ¬ ì‹¤í–‰
- Streaming: Server-Sent Events(SSE)ë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì‘ë‹µ
- Agentic Loop: LLMì´ ì—¬ëŸ¬ ë„êµ¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” ë‹¤ì¤‘ í„´ ì‹¤í–‰
- Memory Management: ëŒ€ìš©ëŸ‰ ë°ì´í„° ìë™ ì ˆë‹¨ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ì˜¤ë²„í”Œë¡œìš° ë°©ì§€

Transport: streamable-http (FastMCP ì„œë²„ì™€ í˜¸í™˜)
"""

import asyncio
import json
import logging
import os
import shutil
import sys
from contextlib import asynccontextmanager
from datetime import datetime
from typing import Any, AsyncGenerator, Dict, List, Optional, Union

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
import uvicorn

# OpenAI Official Client (OpenRouter Support)
from openai import AsyncOpenAI

# MCP Official Client Library
# MCP Official Client Library
from mcp import ClientSession
from mcp.types import Tool
# Note: sse_client is imported inside lifespan to avoid circular imports or runtime errors if not needed

# ============================================================
# ë¡œê¹… ì„¤ì •
# ============================================================
# DEBUG í™˜ê²½ë³€ìˆ˜ê°€ trueë©´ ìƒì„¸ ë¡œê·¸, ì•„ë‹ˆë©´ INFO ë ˆë²¨ë§Œ ì¶œë ¥
LOG_LEVEL = logging.DEBUG if os.getenv("DEBUG", "false").lower() == "true" else logging.INFO
logging.basicConfig(
    level=LOG_LEVEL,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================
# MCP ì„œë²„ URL í—¬í¼ í•¨ìˆ˜
# ============================================================
def get_mcp_server_url() -> str:
    """
    MCP ì„œë²„ URLì„ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    ì¤‘ìš”: FastMCPì˜ streamable-http transportëŠ” /mcp ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ë©°,
    trailing slash(/)ê°€ ìˆìœ¼ë©´ 307 ë¦¬ë‹¤ì´ë ‰íŠ¸ê°€ ë°œìƒí•˜ì—¬ SSE ì—°ê²°ì´ ëŠì–´ì§‘ë‹ˆë‹¤.
    ë”°ë¼ì„œ ë°˜ë“œì‹œ /mcpë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤ (ì˜ˆ: http://superset:5008/mcp)
    """
    return os.getenv("MCP_SERVER_URL", "http://superset:5008/mcp")

# ============================================================
# í™˜ê²½ ì„¤ì •
# ============================================================
# OpenRouter API í‚¤ (LLM í˜¸ì¶œì— í•„ìš”)
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY", "")
# ê¸°ë³¸ ì‚¬ìš© ëª¨ë¸ (gpt-4o ë˜ëŠ” gpt-4o-mini)
DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "openai/gpt-4o")

# ============================================================
# ì „ì—­ ìƒíƒœ ê´€ë¦¬
# ============================================================
class GlobalState:
    """
    ì• í”Œë¦¬ì¼€ì´ì…˜ ì „ì—­ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    
    Attributes:
        session: MCP ClientSession - MCP ì„œë²„ì™€ì˜ ì—°ê²° ì„¸ì…˜
        exit_stack: AsyncExitStack - ë¦¬ì†ŒìŠ¤ ì •ë¦¬ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
    """
    session: Optional[ClientSession] = None
    exit_stack: Optional[Any] = None

state = GlobalState()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ìƒëª…ì£¼ê¸°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
    
    ì´ í•¨ìˆ˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ MCP ì„œë²„ì™€ì˜ ì—°ê²°ì„ ì„¤ì •í•˜ê³ ,
    ì¢…ë£Œ ì‹œ ì—°ê²°ì„ ì •ë¦¬í•©ë‹ˆë‹¤.
    
    ì—°ê²° í”„ë¡œì„¸ìŠ¤:
    1. streamable-http transportë¡œ MCP ì„œë²„ì— ì—°ê²°
    2. ClientSession ìƒì„±
    3. MCP í”„ë¡œí† ì½œ ì´ˆê¸°í™” (initialize ìš”ì²­)
    4. ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ì¡°íšŒ
    
    Transport: streamable-http (FastMCP ì„œë²„ì™€ í˜¸í™˜)
    """
    mcp_url = get_mcp_server_url()
    logger.info("=" * 60)
    logger.info("ğŸš€ Starting MCP Client (streamable-http Transport)")
    logger.info(f"ğŸ“¡ Target MCP Server: {mcp_url}")
    logger.info("=" * 60)
    
    try:
        from contextlib import AsyncExitStack
        from mcp.client.streamable_http import streamable_http_client

        exit_stack = AsyncExitStack()
        
        # ============================================================
        # STEP 1: Connect via streamable-http transport
        # This matches FastMCP's streamable-http server transport
        # ============================================================
        logger.info("ğŸ“¬ STEP 1: Connecting via streamable-http...")
        logger.debug(f"   â†’ Connecting to {mcp_url}")
        
        read_stream, write_stream, get_session_id = await exit_stack.enter_async_context(
            streamable_http_client(mcp_url)
        )
        
        logger.info("âœ“ streamable-http connection established")
        logger.debug(f"   Session ID: {get_session_id() if get_session_id else 'N/A'}")
        
        # ============================================================
        # STEP 2: Create MCP ClientSession
        # ============================================================
        logger.info("ğŸ”§ STEP 2: Creating MCP ClientSession...")
        
        session = await exit_stack.enter_async_context(
            ClientSession(read_stream, write_stream)
        )
        
        logger.info("âœ“ ClientSession created")
        
        # ============================================================
        # STEP 3: Initialize MCP Protocol
        # ============================================================
        logger.info("ğŸ¤ STEP 3: Initializing MCP session...")
        logger.debug("   â†’ Sending initialize request")
        
        await session.initialize()
        
        logger.info("âœ“ Initialize response received")
        
        # Store session in global state
        state.session = session
        state.exit_stack = exit_stack
        
        # ============================================================
        # STEP 4: List Available Tools
        # ============================================================
        logger.info("ğŸ“‹ STEP 4: Requesting tool list...")
        logger.debug("   â†’ Sending tools/list request")
        
        tools = await session.list_tools()
        
        logger.info("âœ“ Tool list received")
        
        # ============================================================
        # SUCCESS: Connection Established
        # ============================================================
        logger.info("=" * 60)
        logger.info(f"âœ… MCP Connection Established!")
        logger.info(f"   Transport: streamable-http")
        logger.info(f"   Available tools: {len(tools.tools)}")
        logger.info("=" * 60)
        
        # Log first few tools
        if tools.tools:
            logger.info("Sample tools:")
            for tool in tools.tools[:5]:
                desc = tool.description[:50] + "..." if len(tool.description) > 50 else tool.description
                logger.info(f"  â€¢ {tool.name}: {desc}")
            if len(tools.tools) > 5:
                logger.info(f"  ... and {len(tools.tools) - 5} more")
        
        logger.info("=" * 60)
        
        yield
        
    except Exception as e:
        logger.error("=" * 60)
        logger.error("âŒ MCP Connection Failed!")
        logger.error(f"   URL: {mcp_url}")
        logger.error(f"   Error: {e}")
        logger.error("=" * 60)
        logger.debug("Full traceback:", exc_info=True)
        
        # Don't raise here, allow the app to start even if MCP is down
        # Health check endpoint will report the connection status
        yield
        
    finally:
        logger.info("ğŸ›‘ Shutting down MCP Client...")
        if state.exit_stack:
            try:
                await state.exit_stack.aclose()
                logger.info("âœ“ MCP connection closed cleanly")
            except Exception as e:
                logger.error(f"Error during shutdown: {e}")


app = FastAPI(
    title="Streaming Superset MCP Client", 
    version="2.2.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================
# ë°ì´í„° ëª¨ë¸ ì •ì˜
# ============================================================

class ChatMessage(BaseModel):
    """
    ì±„íŒ… ë©”ì‹œì§€ ëª¨ë¸ (OpenAI Chat Completion API í˜•ì‹)
    
    Attributes:
        role: ë©”ì‹œì§€ ì—­í•  ("user", "assistant", "tool", "system")
        content: ë©”ì‹œì§€ ë‚´ìš©
        tool_calls: LLMì´ ìš”ì²­í•œ ë„êµ¬ í˜¸ì¶œ ëª©ë¡ (assistant ë©”ì‹œì§€ì—ì„œ ì‚¬ìš©)
        tool_call_id: ë„êµ¬ í˜¸ì¶œ ID (tool ë©”ì‹œì§€ì—ì„œ ì‚¬ìš©)
        name: ë„êµ¬ ì´ë¦„ (tool ë©”ì‹œì§€ì—ì„œ ì‚¬ìš©)
    """
    role: str
    content: Optional[str] = ""
    tool_calls: Optional[List[Dict]] = None
    tool_call_id: Optional[str] = None
    name: Optional[str] = None

class ChatRequest(BaseModel):
    """
    ì±„íŒ… ìš”ì²­ ëª¨ë¸
    
    Attributes:
        messages: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì‚¬ìš©ì ë©”ì‹œì§€ í¬í•¨)
        model: ì‚¬ìš©í•  LLM ëª¨ë¸ (ì˜ˆ: "openai/gpt-4o-mini")
        temperature: ì‘ë‹µì˜ ì°½ì˜ì„± (0.0~1.0, ë†’ì„ìˆ˜ë¡ ì°½ì˜ì )
        max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
    """
    messages: List[ChatMessage]
    model: Optional[str] = DEFAULT_MODEL
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 2000

class StreamChunk(BaseModel):
    """
    ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬ ëª¨ë¸ (Server-Sent Events í˜•ì‹)
    
    Attributes:
        type: ì²­í¬ íƒ€ì…
            - "content": LLM ì‘ë‹µ í…ìŠ¤íŠ¸
            - "tool_start": ë„êµ¬ ì‹¤í–‰ ì‹œì‘
            - "tool_result": ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ
            - "tool_calls": LLMì´ ë„êµ¬ í˜¸ì¶œ ìš”ì²­
            - "error": ì˜¤ë¥˜ ë°œìƒ
            - "done": ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
        content: í…ìŠ¤íŠ¸ ë‚´ìš©
        tool_name: ì‹¤í–‰ ì¤‘ì¸ ë„êµ¬ ì´ë¦„
        tool_result: ë„êµ¬ ì‹¤í–‰ ê²°ê³¼
        error: ì˜¤ë¥˜ ë©”ì‹œì§€
        metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ì˜ˆ: tool_calls ëª©ë¡)
        timestamp: ì²­í¬ ìƒì„± ì‹œê° (ISO 8601 í˜•ì‹)
    """
    type: str
    content: Optional[str] = None
    tool_name: Optional[str] = None
    tool_result: Optional[Dict] = None
    error: Optional[str] = None
    metadata: Optional[Dict] = None
    timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat())

# ============================================================
# OpenRouter í´ë¼ì´ì–¸íŠ¸
# ============================================================

class StreamingOpenRouterClient:
    """
    OpenRouter APIë¥¼ ì‚¬ìš©í•˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° LLM í´ë¼ì´ì–¸íŠ¸
    
    OpenRouterëŠ” OpenAI APIì™€ í˜¸í™˜ë˜ëŠ” ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ë¯€ë¡œ
    OpenAI ê³µì‹ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ì£¼ìš” ê¸°ëŠ¥:
    - ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (Server-Sent Events)
    - Function Calling (ë„êµ¬ í˜¸ì¶œ)
    - ë‹¤ì–‘í•œ LLM ëª¨ë¸ ì§€ì› (GPT-4, Claude, Gemini ë“±)
    """
    
    def __init__(self, api_key: str):
        """
        OpenRouter í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            api_key: OpenRouter API í‚¤ (sk-or-v1-ë¡œ ì‹œì‘)
        """
        self.api_key = api_key
        
        # API í‚¤ ìƒíƒœ ë¡œê¹… (ë””ë²„ê¹…ìš©)
        if not api_key or api_key.strip() == "":
            logger.error("âŒ OpenRouter API key is EMPTY!")
        else:
            logger.info(f"âœ“ OpenRouter API key loaded: {api_key[:10]}...{api_key[-4:]}")
        
        # OpenAI í˜¸í™˜ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        # OpenRouterëŠ” Authorization: Bearer {api_key} í—¤ë”ë¥¼ ì‚¬ìš©
        self.client = AsyncOpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
            default_headers={
                "HTTP-Referer": "http://localhost:8088",  # OpenRouter ìš”êµ¬ì‚¬í•­
                "X-Title": "Superset AI Assistant",       # ì•± ì‹ë³„ìš©
            }
        )
    
    def _convert_mcp_tool_to_openai_function(self, mcp_tool: Tool) -> Dict[str, Any]:
        """
        MCP Toolì„ OpenAI Function Calling í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        MCP Tool í˜•ì‹:
        {
            "name": "list_dashboards",
            "description": "ëŒ€ì‹œë³´ë“œ ëª©ë¡ ì¡°íšŒ",
            "inputSchema": {...}
        }
        
        OpenAI Function í˜•ì‹:
        {
            "type": "function",
            "function": {
                "name": "list_dashboards",
                "description": "ëŒ€ì‹œë³´ë“œ ëª©ë¡ ì¡°íšŒ",
                "parameters": {...}
            }
        }
        
        Args:
            mcp_tool: MCP Tool ê°ì²´
            
        Returns:
            OpenAI Function Calling í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬
        """
        return {
            "type": "function",
            "function": {
                "name": mcp_tool.name,
                "description": mcp_tool.description,
                "parameters": mcp_tool.inputSchema
            }
        }
    
    async def stream_chat_with_tools(
        self, 
        messages: List[Dict[str, Any]], 
        tools: List[Tool], 
        model: str,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> AsyncGenerator[StreamChunk, None]:
        """
        LLMê³¼ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…ì„ ìˆ˜í–‰í•˜ë©°, ë„êµ¬ í˜¸ì¶œì„ ì§€ì›í•©ë‹ˆë‹¤.
        
        ì´ í•¨ìˆ˜ëŠ” OpenAIì˜ Function Calling ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬
        LLMì´ í•„ìš”í•œ ë„êµ¬ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
        
        ìŠ¤íŠ¸ë¦¬ë° í”„ë¡œì„¸ìŠ¤:
        1. LLMì—ê²Œ ë©”ì‹œì§€ì™€ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ì „ë‹¬
        2. LLM ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°
        3. LLMì´ ë„êµ¬ í˜¸ì¶œì„ ìš”ì²­í•˜ë©´ tool_calls ì²­í¬ ë°˜í™˜
        4. ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µì´ë©´ content ì²­í¬ ë°˜í™˜
        
        Args:
            messages: ëŒ€í™” íˆìŠ¤í† ë¦¬
            tools: ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ ëª©ë¡
            model: ì‚¬ìš©í•  LLM ëª¨ë¸
            temperature: ì‘ë‹µì˜ ì°½ì˜ì„± (0.0~1.0)
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            
        Yields:
            StreamChunk: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬
                - type="content": LLM í…ìŠ¤íŠ¸ ì‘ë‹µ
                - type="tool_calls": LLMì´ ë„êµ¬ í˜¸ì¶œ ìš”ì²­
                - type="done": ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
                - type="error": ì˜¤ë¥˜ ë°œìƒ
        """
        
        # API í‚¤ ê²€ì¦
        if not self.api_key:
            yield StreamChunk(type="error", error="OpenRouter API key not configured")
            return
        
        # MCP Toolì„ OpenAI Function í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        openai_tools = [self._convert_mcp_tool_to_openai_function(t) for t in tools] if tools else None
        
        try:
            # OpenAI API í˜¸ì¶œ íŒŒë¼ë¯¸í„° êµ¬ì„±
            kwargs = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": True,  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            }
            
            # ë„êµ¬ê°€ ìˆìœ¼ë©´ Function Calling í™œì„±í™”
            if openai_tools:
                kwargs["tools"] = openai_tools
                kwargs["tool_choice"] = "auto"  # LLMì´ ìë™ìœ¼ë¡œ ë„êµ¬ ì„ íƒ
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
            stream = await self.client.chat.completions.create(**kwargs)
            
            # ë„êµ¬ í˜¸ì¶œ ë²„í¼ (ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì¡°ê°ë‚œ tool_callsë¥¼ ëª¨ìŒ)
            tool_calls = []
            
            # ìŠ¤íŠ¸ë¦¼ ì²­í¬ ì²˜ë¦¬
            async for chunk in stream:
                if not chunk.choices: 
                    continue
                    
                choice = chunk.choices[0]
                delta = choice.delta
                
                # í…ìŠ¤íŠ¸ ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¬ë°
                if delta.content:
                    yield StreamChunk(type="content", content=delta.content)
                
                # ë„êµ¬ í˜¸ì¶œ ìŠ¤íŠ¸ë¦¬ë° (ì¡°ê°ë‚œ ë°ì´í„°ë¥¼ ë²„í¼ì— ëˆ„ì )
                if delta.tool_calls:
                    for tc in delta.tool_calls:
                        # ë²„í¼ í¬ê¸° í™•ì¥
                        while len(tool_calls) <= tc.index:
                            tool_calls.append({
                                "id": "", 
                                "type": "function", 
                                "function": {"name": "", "arguments": ""}
                            })
                        
                        # ì¡°ê°ë‚œ ë°ì´í„° ëˆ„ì 
                        if tc.id: 
                            tool_calls[tc.index]["id"] = tc.id
                        if tc.function.name: 
                            tool_calls[tc.index]["function"]["name"] += tc.function.name
                        if tc.function.arguments: 
                            tool_calls[tc.index]["function"]["arguments"] += tc.function.arguments
                
                # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì²˜ë¦¬
                if choice.finish_reason == "tool_calls":
                    # LLMì´ ë„êµ¬ í˜¸ì¶œ ìš”ì²­
                    yield StreamChunk(type="tool_calls", metadata={"tool_calls": tool_calls})
                    return
                elif choice.finish_reason == "stop":
                    # ì •ìƒ ì¢…ë£Œ
                    yield StreamChunk(type="done")
                    return
                    
            # ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ
            yield StreamChunk(type="done")
                    
        except Exception as e:
            logger.error(f"Streaming failed: {e}")
            yield StreamChunk(type="error", error=str(e))

try:
    streaming_openrouter_client = StreamingOpenRouterClient(OPENROUTER_API_KEY)
except Exception as e:
    logger.error(f"Failed to initialize OpenRouter: {e}")
    streaming_openrouter_client = None

@app.get("/health")
async def health_check():
    """Health check endpoint - returns MCP connection status"""
    is_connected = state.session is not None
    return {
        "status": "healthy" if is_connected else "degraded",
        "transport": "streamable-http",
        "connected": is_connected,
        "message": "MCP session active" if is_connected else "MCP session not connected"
    }

@app.get("/models")
async def list_models():
    return {
        "models": [
            {"id": "openai/gpt-4o", "name": "GPT-4o", "supports_functions": True},
            {"id": "openai/gpt-4o-mini", "name": "GPT-4o Mini", "supports_functions": True},
        ],
        "default": DEFAULT_MODEL
    }

@app.get("/mcp/tools")
async def list_mcp_tools():
    if not state.session:
        raise HTTPException(status_code=503, detail="MCP Session not connected")
    result = await state.session.list_tools()
    return {"tools": result.tools}

@app.post("/chat")
async def chat_completion_stream(request: ChatRequest):
    async def generate():
        if not streaming_openrouter_client or not state.session:
            yield f"data: {json.dumps({'type': 'error', 'error': 'Services not initialized'})}\n\n"
            return
        
        try:
            # Prepare initial messages
            # Convert Pydantic models to dicts for OpenAI API
            current_messages = []
            for m in request.messages:
                msg = {"role": m.role, "content": m.content}
                if m.tool_calls: msg["tool_calls"] = m.tool_calls
                if m.tool_call_id: msg["tool_call_id"] = m.tool_call_id
                if m.name: msg["name"] = m.name
                current_messages.append(msg)
            
            # Agentic Loop
            MAX_LOOPS = 5
            loop_count = 0
            
            while loop_count < MAX_LOOPS:
                loop_count += 1
                
                # Get Tools
                try:
                    tools_result = await state.session.list_tools()
                    mcp_tools = tools_result.tools
                except Exception as e:
                    yield f"data: {json.dumps({'type': 'error', 'error': f'Failed to list tools: {e}'})}\n\n"
                    return

                tool_calls_buffer = []
                full_content_buffer = ""
                
                # Stream Chat from LLM
                async for chunk in streaming_openrouter_client.stream_chat_with_tools(
                    current_messages, mcp_tools, request.model, request.temperature, request.max_tokens
                ):
                    def send(c): return f"data: {c.json()}\n\n"
                    
                    if chunk.type == "tool_calls":
                        tool_calls_buffer = chunk.metadata.get("tool_calls", [])
                    elif chunk.type == "content":
                        full_content_buffer += (chunk.content or "")
                        yield send(chunk)
                    elif chunk.type == "error":
                        yield send(chunk)
                        return
                    # catch 'done' or others but don't break yet
                
                # If no tool calls were generated, we are done
                if not tool_calls_buffer:
                    yield f"data: {json.dumps({'type': 'done'})}\n\n"
                    break
                    
                # If there are tool calls, we need to:
                # 1. Append Assistant Message (with tool calls) to history
                current_messages.append({
                    "role": "assistant",
                    "content": full_content_buffer,
                    "tool_calls": tool_calls_buffer
                })
                
                # 2. Execute Tools
                for tc in tool_calls_buffer:
                    fn = tc["function"]
                    name = fn["name"]
                    call_id = tc["id"]
                    
                    try:
                        args = json.loads(fn["arguments"])
                        
                        yield f"data: {json.dumps({'type': 'tool_start', 'tool_name': name, 'content': f'ğŸ”§ {name} ì‹¤í–‰ ì¤‘...'})}\n\n"
                        
                        # Call Tool via MCP Session
                        result = await state.session.call_tool(name, args)
                        
                        # Format content for History
                        content_str = ""
                        if result.content:
                            for c in result.content:
                                if c.type == "text": content_str += c.text
                                elif c.type == "image": content_str += "[Image]"
                        
                        # Memory Safeguard: Truncate large outputs
                        # This prevents Context Window overflow and Memory Bloat
                        MAX_CHARS = 20000 # Approx 5k tokens
                        if len(content_str) > MAX_CHARS:
                            content_str = content_str[:MAX_CHARS] + f"\n... (Truncated {len(content_str)-MAX_CHARS} chars)"

                        # Append Tool Result to History
                        current_messages.append({
                            "role": "tool",
                            "tool_call_id": call_id,
                            "name": name,
                            "content": content_str
                        })
                        
                        yield f"data: {json.dumps({'type': 'tool_result', 'tool_name': name, 'content': f'âœ… {name} ì™„ë£Œ'})}\n\n"
                        
                    except Exception as e:
                        error_msg = f"Tool execution error: {str(e)}"
                        logger.error(error_msg)
                        
                        # Append Error to History so LLM knows it failed
                        current_messages.append({
                            "role": "tool",
                            "tool_call_id": call_id,
                            "name": name,
                            "content": error_msg
                        })
                        
                        yield f"data: {json.dumps({'type': 'error', 'error': error_msg})}\n\n"
                
                # Loop continues to next iteration (LLM will see tool results and respond)
            
            if loop_count >= MAX_LOOPS:
                yield f"data: {json.dumps({'type': 'error', 'error': 'Agent loop limit reached'})}\n\n"

        except Exception as e:
            logger.error(f"Chat error: {e}")
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)